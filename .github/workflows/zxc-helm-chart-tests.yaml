##
# Copyright (C) 2023-2024 Hedera Hashgraph, LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##

name: "ZXC: FS Helm Chart Tests"
# The purpose of this reusable workflow is to compile the code and run the unit tests on every PR and commit.
# This reusable component is called by the following workflows:
# - .github/workflows/flow-pull-request-checks.yaml

on:
  workflow_call:
    inputs:
      custom-job-label:
        description: "Custom Job Label:"
        type: string
        required: false
        default: "Helm Chart Test"

defaults:
  run:
    shell: bash

permissions:
  id-token: write
  contents: read
  actions: read
  pull-requests: write
  checks: write
  statuses: write

jobs:
  test:
    name: ${{ inputs.custom-job-label || 'Helm Chart Test' }} (${{ matrix.scriptName }})
    runs-on: [self-hosted, Linux, medium, ephemeral]
    strategy:
      fail-fast: false
      matrix:
        # direct-install.sh uses ubi8-init-java17 image
        # nmt-install.sh uses ubi8-init-dind image
        scriptName: [ direct-install.sh, nmt-install.sh  ]
    steps:
      - name: Get changed files related to charts
        id: changed-files
        uses: tj-actions/changed-files@90a06d6ba9543371ab4df8eeca0be07ca6054959 #v42.0.2
        with:
          files_yaml: |
            chart:
              - charts/**
              - .github/workflows/support/**
            scripts:
              - dev/**
              - .github/workflows/support/**

      - name: List all changed files related to charts
        id: check-changed-files
        run: |
          echo "Modified charts files"
          echo "-------------------------------------------------------------------"
          for file in ${{ steps.changed-files.outputs.chart_all_changed_files }}; do
            echo " - ${file} was changed"
          done
          echo ""
          echo "Modified script files"
          echo "-------------------------------------------------------------------"
          for file in ${{ steps.changed-files.outputs.scripts_all_changed_files }}; do
            echo " - ${file} was changed"
          done
          echo ""
          if [ "${{ steps.changed-files.outputs.scripts_any_changed }}" ] || [ "${{ steps.changed-files.outputs.scripts_any_changed }}" ]; then
            echo "run-tests=true" >> "${GITHUB_OUTPUT}"
            echo "Executing helm chart tests...."
          else
            echo ">> No relevant files are changed. No need to run helm chart tests"
          fi

      - name: Initialize Job Variables
        id: vars
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        run: |
          echo "script_name=${{ matrix.scriptName }}" >> "${GITHUB_OUTPUT}"

      - name: Checkout Code
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        with:
          # the fetch depth defaults to only the commit that triggered the workflow unless the spotless check was enabled
          fetch-depth: ${{ inputs.enable-spotless-check && '0' || '' }}

      - name: Docker Prune
        id: docker-prune
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        run: |
          docker system prune -f
          docker image prune -f

      - name: Setup Kind
        uses: helm/kind-action@dda0770415bac9fc20092cacbc54aa298604d140 # v1.8.0
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        with:
          node_image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72
          config: dev/dev-cluster.yaml
          version: v0.20.0
          verbosity: 3
          wait: 120s

      - name: Setup Helm
        uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78 # v3.5
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        with:
          version: "v3.12.3" #  helm version

      # Technically, this step is not required for the unit tests to run, but it is useful for debugging setup issues.
      - name: Kubernetes Cluster Info
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        run: |
          kubectl config get-contexts
          kubectl get crd
          kubectl get node --show-labels

      - name: Helm Chart Test
        working-directory: .github/workflows/support
        if: ${{ steps.check-changed-files.outputs.run-tests && !cancelled() && !failure() }}
        run: |
          echo "-----------------------------------------------------------------------------------------------------"
          echo "Setting up environment variables"
          SCRIPT_NAME="${{ steps.vars.outputs.script_name}}"

          CUR_DIR="scripts"
          source "${CUR_DIR}/env.sh"


          CHART_VALUES_FILES=ci/ci-values.yaml
          SCRIPTS_DIR=scripts

          TELEMETRY_SCRIPT="telemetry.sh"
          GATEWAY_API_SCRIPT="gateway.sh"
          DOCKER_SCRIPT="docker.sh"

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Creating cluster and namespace"
          kind create cluster -n "${CLUSTER_NAME}" --config=dev-cluster.yaml

          kubectl create ns "${NAMESPACE}"
          kubectl get ns
          kubectl config use-context "kind-${CLUSTER_NAME}"
          kubectl config set-context --current --namespace="${NAMESPACE}"
          kubectl config get-contexts

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Building kubectl-bats image"
          KUBECTL_BATS_IMAGE="${LOCAL_DOCKER_REGISTRY}/kubectl-bats:${LOCAL_DOCKER_IMAGE_TAG}"
          cd "${DOCKERFILE_DIR}/kubectl-bats" && docker build -t "${KUBECTL_BATS_IMAGE}" .
          kind load docker-image "${KUBECTL_BATS_IMAGE}" -n "${CLUSTER_NAME}"
          cd -


          echo "-----------------------------------------------------------------------------------------------------"
          echo "Helm dependency update"

          echo "cloud:" > ${CLUSTER_SETUP_VALUES_FILE}
          echo "  prometheusStack:" >> ${CLUSTER_SETUP_VALUES_FILE}; \
          echo "    enabled: true" >> ${CLUSTER_SETUP_VALUES_FILE};
          echo "  minio:" >> ${CLUSTER_SETUP_VALUES_FILE}; \
          echo "    enabled: true" >> ${CLUSTER_SETUP_VALUES_FILE};

          helm dependency update ../charts/fullstack-deployment
          helm dependency update ../charts/fullstack-cluster-setup

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Helm cluster setup"

          helm install -n "${NAMESPACE}" "fullstack-cluster-setup" "${SETUP_CHART_DIR}" --values "${CLUSTER_SETUP_VALUES_FILE}"
          echo "-----------------------Shared Resources------------------------------------------------------------------------------"
          kubectl get clusterrole "${POD_MONITOR_ROLE}" -o wide


          echo ""
          echo "Installing helm chart... "
          echo "SCRIPT_NAME: ${SCRIPT_NAME}"
          echo "Additional values: ${CHART_VALUES_FILES}"
          echo "-----------------------------------------------------------------------------------------------------"
          if [ "${SCRIPT_NAME}" = "nmt-install.sh" ]; then
            if [[ -z "${CHART_VALUES_FILES}" ]]; then
              helm install "${RELEASE_NAME}" -n "${NAMESPACE}" "${CHART_DIR}" --set defaults.root.image.repository=hashgraph/full-stack-testing/ubi8-init-dind
            else
              helm install "${RELEASE_NAME}" -n "${NAMESPACE}"  "${CHART_DIR}" -f "${CHART_DIR}/values.yaml" --values "${CHART_VALUES_FILES}" --set defaults.root.image.repository=hashgraph/full-stack-testing/ubi8-init-dind
            fi
          else
            if [[ -z "${CHART_VALUES_FILES}" ]]; then
              helm install "${RELEASE_NAME}" -n "${NAMESPACE}" "${CHART_DIR}"
            else
              helm install "${RELEASE_NAME}" -n "${NAMESPACE}" "${CHART_DIR}" -f "${CHART_DIR}/values.yaml" --values "${CHART_VALUES_FILES}"
            fi
          fi

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Get service and pod information"

          kubectl get svc -o wide && \
          kubectl get pods -o wide && \

          echo "Waiting for network-node pods to be active (first deployment takes ~10m)...."
          kubectl wait --for=jsonpath='{.status.phase}'=Running pod -l fullstack.hedera.com/type=network-node --timeout=900s

          echo "Service Information...."
          kubectl get svc -o wide

          echo "Waiting for pods to be up (timeout 600s)"
          kubectl wait --for=jsonpath='{.status.phase}'=Running pod -l fullstack.hedera.com/type=network-node --timeout=600s


          echo "Running helm chart tests (takes ~5m, timeout 15m)... "
          echo "-----------------------------------------------------------------------------------------------------"

          helm test "${RELEASE_NAME}" --filter name=network-test --timeout 15m

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Setup and start nodes"
          source "${SCRIPTS_DIR}/${SCRIPT_NAME}" && setup_node_all
          source "${SCRIPTS_DIR}/${SCRIPT_NAME}" && start_node_all

          echo "-----------------------------------------------------------------------------------------------------"
          echo "Tear down cluster"

          kubectl delete pod network-test -n "${NAMESPACE}" || true

          echo "Uninstalling helm chart ${RELEASE_NAME} in namespace ${NAMESPACE}... "
          echo "-----------------------------------------------------------------------------------------------------"
          helm uninstall -n "${NAMESPACE}" "${RELEASE_NAME}"
          sleep 10
          echo "Uninstalled helm chart ${RELEASE_NAME} in namespace ${NAMESPACE}"

          echo "Removing postgres pvc"
          has_postgres_pvc=$(kubectl get pvc --no-headers -l app.kubernetes.io/component=postgresql,app.kubernetes.io/name=postgres,app.kubernetes.io/instance="${RELEASE_NAME}" | wc -l)
          if [[ $has_postgres_pvc ]]; then
            kubectl delete pvc -l app.kubernetes.io/component=postgresql,app.kubernetes.io/name=postgres,app.kubernetes.io/instance="${RELEASE_NAME}"
          fi

          echo "Workflow finished successfully"
          echo "-----------------------------------------------------------------------------------------------------"
