# Default values for network-node.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/hashgraph/full-stack-testing/ubi8-init-dind
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.1.2"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
#  fsGroup: 2000 # hedera group

securityContext:
#  capabilities:
#    drop:
#      - ALL
#  readOnlyRootFilesystem: true
#  runAsNonRoot: false
#  runAsUser: 0 # hedera user
#  runAsGroup: 0 # hedera group
#  allowPrivilegeEscalation: true
  privileged: true

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: hedera-node.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
   limits:
     cpu: 100m
     memory: 128Mi
   requests:
     cpu: 100m
     memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

podVolumes:
  - name: data
    emptyDir: {}
  - name: otel-collector-config
    configMap:
      name: otel-collector-config

volumes:
  - name: data
    mountPath: /app/data

affinity: {}

livenessProbe: {}
#  httpGet:
#    path: /
#    port: http

#readinessProbe:
#  httpGet:
#    path: /
#    port: http

# sidecar configurations
sidecars:
  otel:
    enabled: true
    image:
      repository: "otel/opentelemetry-collector-contrib"
      tag: "0.72.0"
      pullPolicy: IfNotPresent
    resources:
#      limits:
#        cpu: 100m
#        memory: 128Mi
#      requests:
#        cpu: 100m
#        memory: 128Mi
    ports:
#      otlp:
#        enabled: true
#        containerPort: 4317
#        protocol: TCP
#      metrics:
#        enabled: true
#        containerPort: 8888
#        protocol: TCP
#      healthcheck:
#        enabled: true
#        containerPort: 13133
#        protocol: TCP
    volumes:
     - name: otel-collector-config
       mountPath: /etc/otel-collector-config.yaml
       subPath: config.yaml #key in the configmap
       readOnly: true
    configFile: "" # need to be loaded by helm using --set-file
